{
 "metadata": {
  "name": "",
  "signature": "sha256:efb2f890863d9c967272c83859d4ae19f5a43752aebd7f73144d44237e0af13b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Job register web scraping, \n",
      "\n",
      "## I. Scraping the web\n",
      "\n",
      "### Michael Gully-Santiago, October 2, 2014\n",
      "\n",
      "Basically I'm scaping the Job Register website to compile all the job add into into a single Excel file.\n",
      "\n",
      "I am following [this example](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/) from Greg Reda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "#import time\n",
      "#import pandas as pd\n",
      "from astropy.table import Table, Column\n",
      "import numpy as np\n",
      "#import copy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = \"https://jobregister.aas.org/\"\n",
      "html = urlopen(BASE_URL).read()\n",
      "soup = BeautifulSoup(html, \"lxml\")\n",
      "pppcp2 = soup.find(\"div\", \"panel-pane pane-custom pane-2\")\n",
      "paneContent = pppcp2.find(\"div\", \"pane-content\")\n",
      "pcTab = paneContent.find(\"table\")\n",
      "allRows = pcTab.findAll(\"tr\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lordList = []\n",
      "for row in allRows:\n",
      "    td = row.find(\"td\")\n",
      "    if (td != None):\n",
      "        link = td.a[\"href\"]\n",
      "        lordList.append(BASE_URL+link)\n",
      "\n",
      "print 'There are ', len(lordList), ' jobs listed on the AAS job register.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are  213  jobs listed on the AAS job register.\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## woohoo, it works!!\n",
      "\n",
      "The next step is to now go to each page and scrape the desired information.\n",
      "\n",
      "By the way, you'll notice that it's not sorted by the job category (postdocs, Faculty, etc).  This is OK because there is a \"Job Category\" listing that we can use to sort things out later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thisHtml = urlopen(thisLink).read()\n",
      "soup = BeautifulSoup(thisHtml, \"lxml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's define the strategy for extracting each element.  We are sticking to the DRY- \"Don't repeat yourself\" programming style, which is the right way to do things."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_and_format_AAS_sibling_entry(cup_of_soup, sub_tag_name):\n",
      "    entry = \"---\"\n",
      "    thisTag = cup_of_soup.find('div',sub_tag_name)\n",
      "    if (thisTag != None):\n",
      "        thisLabel = thisTag.find(\"div\", \"field-label-inline-first\")\n",
      "        sibling = thisLabel.next_sibling\n",
      "        formatted_content = re.sub(' +',' ',sibling).encode('utf-8', 'ignore').replace(\"\\r\\n\", \"\")\n",
      "        entry = unicode(formatted_content, errors='ignore')\n",
      "    return entry"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## We have to define the fields\n",
      "\n",
      "Unfortunately not every job ad has every field.  This is a *missing data problem*.  Because of this missing data, we can't merely say \"find all elements in this tag\", because we wouldn't necessarily know which-one-is-which.  So we have to individually check for each tag, and then enter the value or \"N/A\" if `None`.\n",
      "\n",
      "Unfortuntately I have not exploited clever python looping, zipping, vectorization, etc.  I just repeated over and over the same calls."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Job Details\n",
      "institute_field = \"field field-type-text field-field-institution-name\"\n",
      "jobCat_field = \"field field-type-text field-field-job-category\"\n",
      "\n",
      "# Submission Address for Resumes/CVs\n",
      "attn_to_field = 'field field-type-text field-field-attention-to'\n",
      "attn_to_title_field = 'field field-type-text field-field-attention-to-title'\n",
      "attn_to_org_field = 'field field-type-text field-field-attention-to-rganization' #[sic]\n",
      "attn_to_address_field = 'field field-type-text field-field-attention-to-street-addres' #[sic]\n",
      "attn_to_city_field = 'field field-type-text field-field-attention-to-city'   \n",
      "attn_to_state_field = 'field field-type-text field-field-attention-state-province'\n",
      "attn_to_zip_field = 'field field-type-text field-field-zip-postal-code'         \n",
      "attn_to_country_field = 'field field-type-text field-field-attention-to-country' \n",
      "attn_to_email_field = 'field field-type-text field-field-attention-to-email'\n",
      "\n",
      "# Inquiries\n",
      "inquiry_email_field = \"field field-type-text field-field-inquirie-email\" #[sic]\n",
      "\n",
      "# Desired columns:\n",
      "PostDate = []\n",
      "Deadline = []\n",
      "JobCategory = []\n",
      "Institution = []\n",
      "attn_to = []\n",
      "attn_to_title = []\n",
      "attn_to_org = []\n",
      "attn_to_address = []\n",
      "attn_to_city = []\n",
      "attn_to_state = []\n",
      "attn_to_zip = []\n",
      "attn_to_country = []\n",
      "attn_to_email_field = []\n",
      "inquiry_email = []\n",
      "\n",
      "#this is a bad coding strategy because the memory/entry will be that of the largest string:\n",
      "announce = [] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Big for loop for all the jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Just deal with a subset at first\n",
      "subLordList = lordList\n",
      "\n",
      "i = 0\n",
      "\n",
      "for webLink in subLordList:\n",
      "    i+=1\n",
      "    #print i\n",
      "    if ((i % 10) == 0):\n",
      "        print i\n",
      "    thisHtml = urlopen(webLink).read()\n",
      "    soup = BeautifulSoup(thisHtml, \"lxml\")\n",
      "    #time.sleep(1)\n",
      "    \n",
      "    # ---Submission Dates---\n",
      "    # n.b non-standard extraction strategy here.\n",
      "    gsd = soup.find(\"fieldset\", \"fieldgroup group-submission-dates\")\n",
      "    dds = gsd.findAll(\"span\",\"date-display-single\")\n",
      "    \n",
      "    PostDate.append(str(dds[0].contents[0]))\n",
      "    Deadline.append(str(dds[2].contents[0]))\n",
      "    \n",
      "    # ---Job Details---\n",
      "    gjd = soup.find(\"fieldset\", \"fieldgroup group-job-details\")\n",
      "\n",
      "    JobCategory.append(extract_and_format_AAS_sibling_entry(gjd,jobCat_field))\n",
      "    Institution.append(extract_and_format_AAS_sibling_entry(gjd,institute_field))\n",
      "    \n",
      "    # ---Submission Address for Resumes/CVs---\n",
      "    gsa = soup.find(\"fieldset\", \"fieldgroup group-submission-address\")\n",
      "\n",
      "    attn_to.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_field))\n",
      "    attn_to_title.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_title_field))\n",
      "    attn_to_org.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_org_field))\n",
      "    attn_to_address.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_address_field))\n",
      "    attn_to_city.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_city_field))\n",
      "    attn_to_state.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_state_field))\n",
      "    attn_to_zip.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_zip_field))\n",
      "    attn_to_country.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_country_field))\n",
      "    attn_to_email_field.append(extract_and_format_AAS_sibling_entry(gsa, attn_to_email_field))\n",
      "    \n",
      "    # ---Contact Information For Inquiries about the Job---\n",
      "    gin = soup.find(\"fieldset\", \"fieldgroup group-inquiries\")\n",
      "    \n",
      "    if (gin != None):\n",
      "        inquiry_email.append(extract_and_format_AAS_sibling_entry(gin, inquiry_email_field))\n",
      "    else:\n",
      "        inquiry_email.append(unicode('---'))\n",
      "    \n",
      "    # Announcement \n",
      "    # nb. Slightly different parsing than the others above\n",
      "    gga = soup.find(\"fieldset\", \"fieldgroup group-announcement\")\n",
      "    ann_tag = gga.find('div', 'field-items')\n",
      "    announce_raw = ann_tag.getText().encode('utf-8', 'ignore')\n",
      "    thisAnnounce = unicode(announce_raw, errors='ignore')\n",
      "\n",
      "    announce.append(thisAnnounce)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "110"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "120"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "140"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "170"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "180"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_arr  = [PostDate,\n",
      "            Deadline, \n",
      "            JobCategory,\n",
      "            Institution,\n",
      "            attn_to,\n",
      "            attn_to_title,\n",
      "            attn_to_org,\n",
      "            attn_to_address,\n",
      "            attn_to_city,\n",
      "            attn_to_state,\n",
      "            attn_to_zip,\n",
      "            attn_to_country,\n",
      "            attn_to_email_field,\n",
      "            inquiry_email,\n",
      "            announce]\n",
      "out_names = ('PostDate',\n",
      "            'Deadline',\n",
      "            'JobCategory',\n",
      "            'Institution',\n",
      "            'attn_to',\n",
      "            'attn_to_title',\n",
      "            'attn_to_org',\n",
      "            'attn_to_address',\n",
      "            'attn_to_city',\n",
      "            'attn_to_state',\n",
      "            'attn_to_zip',\n",
      "            'attn_to_country',\n",
      "            'attn_to_email_field',\n",
      "            'inquiry_email',\n",
      "            'announce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = Table(out_arr, names = out_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t.show_in_browser(jsviewer = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "<open file '<fdopen>', mode 'w+b' at 0x111014a50>"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save it to a csv file so we don't have to continually ping the website while we play with the existing data.  We will go back to ping the website for more attributes later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Must use unconventional delimiter because there are commas in the corpus\n",
      "t.write('AllAASjobReg.dat', format='ascii', delimiter=';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's it!  We'll make another IPython notebook to read in the .dat file and play with it."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}